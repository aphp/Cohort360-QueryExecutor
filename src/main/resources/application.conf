# Flyway locations to scan recursively for migrations
flyway.locations = "db/postgresql/migration"
flyway.initOnMigrate = false


#########
# spark #
#########

spark {
  home = "${SPARK_HOME}"
  master = "local[*]"
  submit.deployMode = "client"
  # For more details see https://github.com/spark-jobserver/spark-jobserver/tree/master/doc/supervise-mode.md
  driver.supervise = false

  # spark web UI port
  webUrlPort = 8080

  contexts {
    shared {
      num-cpu-cores = 10 # shared tasks work best in parallel.
      spark.sql.crossJoin.enabled = true
      memory-per-node = 100G  # trial-and-error discovered memory per node
      spark.executor.instances = 17 # 4 r3.xlarge instances with 4 cores each = 16 + 1 master
      spark.scheduler.mode = "FAIR" # plusieurs en parallelle
      context-factory = spark.jobserver.context.SessionContextFactory
      spark.driver.maxResultSize = 20G
     }
    }

  # Universal context configuration.  These settings can be overridden, see README.md
  context-settings {
    spark.ui.port=4040
    num-cpu-cores = 5          # Number of cores to allocate.  Required.
    memory-per-node = 1024m         # Executor memory per node, -Xmx style eg 512m, #1G, etc.
    spark.yarn.queue = etl
    spark.executor.instances = 30
    context-init-timeout = 10000 s

    # A zero-arg class implementing spark.jobserver.context.SparkContextFactory
    # Determines the type of jobs that can run in a SparkContext
    context-factory = spark.jobserver.context.DefaultSparkContextFactory

    # By default Hive support is enabled for SparkSession in jobserver.
    # This property can be used to disable Hive support.
    spark.session.hive.enabled = true

    # Timeout for forked JVM to spin up and acquire resources
    forked-jvm-init-timeout = 30s

    # Timeout for SupervisorActor to wait for forked (separate JVM) contexts to initialize
    context-init-timeout = 60s

     # streaming {
     #      # Default batch interval for Spark Streaming contexts in milliseconds
     #      batch_interval = 1000
     #
     #      # if true, stops gracefully by waiting for the processing of all received data to be completed
     #      stopGracefully = true
     #
     #      # if true, stops the SparkContext with the StreamingContext. The underlying SparkContext will be
     #      # stopped regardless of whether the StreamingContext has been started.
     #      stopSparkContext = true
     #    }

    # In case spark distribution should be accessed from HDFS (as opposed to being installed on every Mesos slave)
    # spark.executor.uri = "hdfs://namenode:8020/apps/spark/spark.tgz"

    # URIs of Jars to be loaded into the classpath for this context.
    # Uris is a string list, or a string separated by commas ','
    # dependent-jar-uris = ["file:///some/path/present/in/each/mesos/slave/somepackage.jar"]

    # Add settings you wish to pass directly to the sparkConf as-is such as Hadoop connection
    # settings that don't use the "spark." prefix
    passthrough {
      cohort360.solr.zk = ${SOLR_ZK}
      cohort360.solr.max_try = 1
      cohort360.solr.rows = 10000
      cohort360.solr.commit_within = 10000

      cohort360.postgres.host = ${PG_HOST}
      cohort360.postgres.database = ${PG_DB}
      cohort360.postgres.schema = ${PG_SCHEMA}
      cohort360.postgres.user = ${PG_USER}
      cohort360.postgres.port = ${PG_PORT}

      # set to 1 if you want to enable
      cohort360.spark.enableCache = 0
    }
    launcher {
      spark.driver.memory = 1g
    }
  }

  ###################
  # spark-jobserver #
  ###################
  jobserver {
    port = 8090
    bind-address = "0.0.0.0"
    jar-store-rootdir = /opt/spark-jobserver/jars
    filedao.rootdir = /opt/spark-jobserver/filedao/data

    # If true, a separate JVM is forked for each Spark context
    context-per-jvm = false
    kill-context-on-supervisor-down = false
    manager-initialization-timeout = 40s

    # Number of job results to keep per JobResultActor/context
    job-result-cache-size = 5000

    # The ask pattern timeout for Api
    short-timeout = 300 s

    # Time out for job server to wait while creating contexts
    context-creation-timeout = 60 s

    # Time out for job server to wait while creating named objects
    named-object-creation-timeout = 60 s

    # Number of jobs that can be run simultaneously per context
    # If not set, defaults to number of cores on machine where jobserver is running
    max-jobs-per-context = 500

    # in yarn deployment, time out for job server to wait while creating contexts
    yarn-context-creation-timeout = 600 s

    # To load up job jars on startup, place them here,
    # with the app name as the key and the path to the jar as the value
    job-bin-paths {
       omop-spark-job = lib/cohort-requester.jar
    }

    # Start embedded H2 Server (usefull in cluster deployment)
    startH2Server = false

    # Health Check Class to be invoked for healthz API
    healthcheck = spark.jobserver.util.APIHealthCheck

    dao-timeout = 600s
    # At jobserver startup, a cleanup class can be provided which cleans the dao.
    # Currently, only ZK implementation is provided.
    # Empty string means disabled, full class path means enabled.
    # Note: For H2, migrations should be used.
    startup_dao_cleanup_class = ""
    # Cache binary on upload.
    # If set to false, the binary will be cached on job start only.
    cache-on-upload = true
    daorootdir = "/tmp/jobserver-dao"
    binarydao {
      class = spark.jobserver.io.SqlBinaryObjectsDAO
    }
    metadatadao {
      class = spark.jobserver.io.MetaDataSqlDAO
    }
    datadao {
      # storage directory for files that are uploaded to the server
      # via POST/data commands
      rootdir = /tmp/spark-jobserver/upload
    }
    # zookeeperdao {
    #   dir = "jobserver/db"
    #   connection-string = "localhost:2181"
    #   curator {
    #     retries = 3
    #     # settings below are chosen for the cluster with 3 Zookeeper nodes
    #     sleepMsBetweenRetries = 1000
    #     connectionTimeoutMs = 2350
    #     sessionTimeoutMs = 10000
    #   }
    #   autopurge = false
    #   autopurge_after_hours = 168
    # }
    sqldao {
      # Slick database driver, full classpath
      slick-driver = slick.jdbc.PostgresProfile

      # JDBC driver, full classpath
      jdbc-driver = org.postgresql.Driver

      # Directory where default H2 driver stores its data. Only needed for H2.
      #rootdir = /export/home/edsprod/spark/job-server/sqldao/data

      # Full JDBC URL / init string, along with username and password.  Sorry, needs to match above.
      # Substitutions may be used to launch job-server, but leave it out here in the default or tests won't pass
      jdbc {
        url = "jdbc:postgresql://${SJS_PG_HOST}:${SJS_PG_PORT}/${SJS_PG_DB}"
        user = "${SJS_PG_USER}"
        password = "${SJS_PG_PASSWORD}"
      }

      # DB connection pool settings
      dbcp {
        enabled = false
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }

    # For cluster mode, the following modes can be used to set
    # akka.remote.netty.tcp.hostname
    # - akka - Let Akka decide based on hostname
    # - manual - SJS uses whatever value is specified in hostname.
    # - auto - Use SPARK_LOCAL_IP or automatically find externally accessible IPv4
    network-address-resolver = "akka"


    # - Empty list of master means HA mode is not enabled.
    # - Specify the masters as a comma seperated list of format IP:PORT,IP2:PORT
    #   - Try to keep the PORT for master equal to the config setting akka.remote.netty.tcp.port
    # - On all masters, the order should be the same for the following property
    # - Starting master nodes is out of scope of jobserver, external entity should start all nodes
    # - HA setup is best suited with a HA dao layer. Currently, combined dao with HDFS + ZK is preferable.
    # - HA setup can also be configured for client/cluster/supervise mode
    #   - For supervise mode, akka.remote.netty.tcp.port setting should be hardcoded. See doc/supervise-mode.md
    #   - For cluster mode, see doc/cluster.md and configure this property
    #   - For client mode, use hardcoded akka.remote.netty.tcp.port instead of random
    ha {
      masters = []
    }

  }
}

# uncomment the following line and enable shiro auth to activate basic auth
# access-control {
#   # note that ssl-encryption should also be on when authentication/authorization is on to avoid that passwords
#   # can be sniffed out
#   # Authentication provider class. Implementations have to extend SJSAuthenticator
#   provider = spark.jobserver.auth.AllowAllAccessControl
#   # Time out for job server to wait for authentication requests
#   auth-timeout = 10 s
#   shiro {
#     # absolute path to shiro config file, including file name
#     config.path = "/some/path/shiro.ini"
#     # when authentication is on, then this flag controls whether the authenticated user
#     # is always used as the proxy-user when starting contexts
#     # (note that this will overwrite the spark.proxy.user parameter that may be provided to the POST /contexts/new-context command)
#     use-as-proxy-user = off
#   }

#   keycloak {
#     authServerUrl = "https://example.com/auth"
#     realmName = "realm"
#     client = "client"
#     clientSecret = "clientSecret"
#   }

#   # If turned on, access-control for authorized users is cached to improve performance. While a user is cached, the
#   # provided password is *not* validated introducing a potential security threat.
#   use-cache = false
#   akka.http.caching.lfu-cache {
#     max-capacity = 512
#     initial-capacity = 16
#     time-to-live = 15 minutes
#     time-to-idle = 1 minutes
#   }
# }

# check the reference.conf in spray-can/src/main/resources for all defined settings
# spray.can.server {
#   ## uncomment the next lines for making this an HTTPS example
#   # ssl-encryption = on
#   # keystore = "/some/path/server-keystore.jks"
#   # keystorePW = "changeit"
#   encryptionType = "SSL"
#   keystoreType = "JKS"
#   provider = "SunX509"
#   #
#   ## Client Authentication (optional): activated upon providing a truststore file
#   # truststore = "/some/path/server-truststore.jks"
#   # truststorePW = "changeit"
#   truststore-type = "JKS"
#   truststore-provider = "SunX509"
#   #
#   # see http://docs.oracle.com/javase/7/docs/technotes/guides/security/StandardNames.html#SSLContext for more examples
#   # typical are either SSL or TLS
#   # key manager factory provider
#   # ssl engine provider protocols
#   enabledProtocols = ["SSLv3", "TLSv1"]
#   idle-timeout = 6000 s
#   request-timeout = 4000 s
#   pipelining-limit = 2 # for maximum performance (prevents StopReading / ResumeReading messages to the IOBridge)
#   # Needed for HTTP/1.0 requests with missing Host headers
#   default-host-header = "spray.io:8765"
#
#   parsing.max-content-length = 300000m
# }

akka {
  # Use SLF4J/logback for deployed environment logging
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    warn-about-java-serializer-usage = off
    serializers {
      customStartJobSerializer = "spark.jobserver.util.StartJobSerializer"
    }
    serialization-bindings {
      "spark.jobserver.JobManagerActor$StartJob" = customStartJobSerializer
    }
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
      send-buffer-size = 20 MiB
      receive-buffer-size = 20 MiB
      # This controls the maximum message size, including job results, that can be sent
      maximum-frame-size = 10 MiB
    }
  }

  cluster {
    auto-down-unreachable-after = 20s
    metrics.enabled = off
    failure-detector.acceptable-heartbeat-pause = 3s
    failure-detector.threshold = 12.0
    allow-weakly-up-members = "off"
  }

  cluster.singleton {
    singleton-name = "context-supervisor"
    role = "supervisor"
  }

  cluster.singleton-proxy {
    singleton-name = ${akka.cluster.singleton.singleton-name}
    role = ${akka.cluster.singleton.role}
    singleton-identification-interval = 1s
    buffer-size = 1000
  }

  coordinated-shutdown {
    run-by-jvm-shutdown-hook = "off"
  }

}

custom-downing {
  stable-after = 20s
  down-removal-margin = 1s

  oldest-auto-downing {
    preferred-oldest-member-role = ${akka.cluster.singleton.role}
  }
}

# check the reference.conf at https://doc.akka.io/docs/akka-http/current/configuration.html for all defined settings
akka.http.server {
  ## uncomment the next lines for making this an HTTPS example
  # ssl-encryption = on
  # keystore = "/some/path/server-keystore.jks"
  # keystorePW = "changeit"
  encryptionType = "SSL"
  keystoreType = "JKS"
  provider = "SunX509"
  #
  ## Client Authentication (optional): activated upon providing a truststore file
  # truststore = "/some/path/server-truststore.jks"
  # truststorePW = "changeit"
  truststore-type = "JKS"
  truststore-provider = "SunX509"
  #
  # see http://docs.oracle.com/javase/7/docs/technotes/guides/security/StandardNames.html#SSLContext for more examples
  # typical are either SSL or TLS
  # key manager factory provider
  # ssl engine provider protocols
  enabledProtocols = ["SSLv3", "TLSv1"]
  idle-timeout = 600 s
  request-timeout = 600 s
  pipelining-limit = 2 # for maximum performance (prevents StopReading / ResumeReading messages to the IOBridge)
  # Needed for HTTP/1.0 requests with missing Host headers
  default-host-header = "spray.io:8765"

  # Increase this in order to upload bigger job jars
  parsing.max-content-length = 30m
}

